{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11e58cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/mnt/e/collegeitem/third1/machinelearning/Fuxi-alpha-in-ReChorus')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34275a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "hasattr(F, \"rms_norm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ebb80ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/e/collegeitem/third1/machinelearning/Fuxi-alpha-in-ReChorus/ReChorus\n"
     ]
    }
   ],
   "source": [
    "%cd ReChorus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38bf500d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = \"/mnt/e/collegeitem/third1/machinelearning/Fuxi-alpha-in-ReChorus/models_saved\"\n",
    "LOG_DIR = \"/mnt/e/collegeitem/third1/machinelearning/Fuxi-alpha-in-ReChorus/log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47aeb51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMON_ML1M_L200 = (\n",
    "    \"--dataset Grocery_and_Gourmet_Food --path data \"     # 或者你自己的 data 路径\n",
    "    \"--history_max 200 \"\n",
    "    \"--gpu 0 \"\n",
    "    \"--lr 1e-3 --l2 0 \"\n",
    "    \"--epoch 101 --early_stop 0 \"                     # early_stop 你可以照以前的 10\n",
    "    \"--batch_size 128 --eval_batch_size 128 \"\n",
    "    \"--num_neg 128 \"                                   # SampledSoftmax 的负样本数\n",
    "    \"--dropout 0.2 \"                                   # 对应 train_fn.dropout_rate\n",
    "    \"--metric NDCG,HR --topk 5,10,20,50 \"\n",
    "    \"--emb_size 50 \"                                   # item_embedding_dim\n",
    "    \"--fuxi_linear_activation silu \"\n",
    "    \"--fuxi_linear_dropout 0.2 --fuxi_attn_dropout 0.2 \"\n",
    "    \"--fuxi_ffn_multiply 1.0 --fuxi_ffn_single_stage 0 \"\n",
    "    \"--fuxi_enable_rel_bias 1 \"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 普通 FuXi (2 blocks, 1 head, dqk=dv=50)\n",
    "model_path = f\"{MODEL_DIR}/fuxi_upgrade_grocery_large.pt\"\n",
    "log_file   = f\"{LOG_DIR}/fuxi_upgrade_grocery_large.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdce5560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/e/collegeitem/third1/machinelearning/Fuxi-alpha-in-ReChorus/ReChorus/src/main.py:25: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"bool\"):\n",
      "Namespace(model_name='fuxi_upgrade', model_mode='')\n",
      "--------------------------------------------- BEGIN: 2025-12-03 00:01:48 ---------------------------------------------\n",
      "\n",
      "===============================================\n",
      " Arguments              | Values               \n",
      "===============================================\n",
      " batch_size             | 128                 \n",
      " data_appendix          |                     \n",
      " dataset                | Grocery_and_Gourm...\n",
      " dropout                | 0.2                 \n",
      " early_stop             | 0                   \n",
      " emb_size               | 50                  \n",
      " epoch                  | 101                 \n",
      " eval_batch_size        | 128                 \n",
      " fuxi_attention_dim     | 25                  \n",
      " fuxi_attn_dropout      | 0.2                 \n",
      " fuxi_blocks            | 8                   \n",
      " fuxi_enable_rel_bias   | 1                   \n",
      " fuxi_ffn_multiply      | 1.0                 \n",
      " fuxi_ffn_single_stage  | 0                   \n",
      " fuxi_heads             | 2                   \n",
      " fuxi_linear_activation | silu                \n",
      " fuxi_linear_dim        | 25                  \n",
      " fuxi_linear_dropout    | 0.2                 \n",
      " gpu                    | 0                   \n",
      " history_max            | 200                 \n",
      " l2                     | 0.0                 \n",
      " lr                     | 0.001               \n",
      " main_metric            |                     \n",
      " num_neg                | 128                 \n",
      " num_workers            | 5                   \n",
      " optimizer              | Adam                \n",
      " random_seed            | 0                   \n",
      " save_final_results     | 0                   \n",
      " test_all               | 0                   \n",
      " topk                   | 5,10,20,50          \n",
      "===============================================\n",
      "Device: cuda\n",
      "Load corpus from data/Grocery_and_Gourmet_Food/SeqReader.pkl\n",
      "Initialize _item_emb.weight as truncated normal: torch.Size([8715, 50]) params\n",
      "#params: 610440\n",
      "FuXiUpgrade(\n",
      "  (embedding_module): LocalEmbeddingModule(\n",
      "    (_item_emb): Embedding(8715, 50, padding_idx=0)\n",
      "  )\n",
      "  (interaction_module): DotProductSimilarity()\n",
      "  (input_preproc): LearnablePositionalEmbeddingInputFeaturesPreprocessor(\n",
      "    (_pos_emb): Embedding(201, 50)\n",
      "    (_emb_dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (output_postproc): L2NormEmbeddingPostprocessor()\n",
      "  (encoder): SemanticFreeFuXi(\n",
      "    (_ndp_module): DotProductSimilarity()\n",
      "    (_embedding_module): LocalEmbeddingModule(\n",
      "      (_item_emb): Embedding(8715, 50, padding_idx=0)\n",
      "    )\n",
      "    (_input_features_preproc): LearnablePositionalEmbeddingInputFeaturesPreprocessor(\n",
      "      (_pos_emb): Embedding(201, 50)\n",
      "      (_emb_dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (_output_postproc): L2NormEmbeddingPostprocessor()\n",
      "    (_fuxi): FuXiJagged(\n",
      "      (_attention_layers): ModuleList(\n",
      "        (0-7): 8 x SemanticFreeFuXiBlock(\n",
      "          (_rel_attn_bias): SeperatedRelativeBucketedTimeAndPositionBasedBias()\n",
      "          (_mffn): MultistageFeedforwardNeuralNetwork(\n",
      "            (lin0): Linear(in_features=100, out_features=50, bias=True)\n",
      "            (lin1): Linear(in_features=50, out_features=50, bias=False)\n",
      "            (lin2): Linear(in_features=50, out_features=50, bias=False)\n",
      "            (lin3): Linear(in_features=50, out_features=50, bias=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Test Before Training: (HR@5:0.0507,NDCG@5:0.0295,HR@10:0.1036,NDCG@10:0.0464,HR@20:0.2071,NDCG@20:0.0722,HR@50:0.5053,NDCG@50:0.1303)\n",
      "Optimizer: Adam\n",
      "Epoch 1     loss=0.5996 [113.1 s]\tdev=(HR@5:0.2835,NDCG@5:0.1887) [4.8 s] *                         \n",
      "Epoch 2     loss=0.5169 [112.8 s]\tdev=(HR@5:0.3470,NDCG@5:0.2376) [4.6 s] *                         \n",
      "Epoch 3     loss=0.4890 [108.1 s]\tdev=(HR@5:0.3679,NDCG@5:0.2540) [4.2 s] *                         \n",
      "Epoch 4     loss=0.4680 [109.7 s]\tdev=(HR@5:0.3918,NDCG@5:0.2799) [4.4 s] *                         \n",
      "Epoch 5     loss=0.4482 [110.7 s]\tdev=(HR@5:0.4064,NDCG@5:0.2962) [7.2 s] *                         \n",
      "Epoch 6     loss=0.4313 [110.7 s]\tdev=(HR@5:0.4180,NDCG@5:0.3080) [4.4 s] *                         \n",
      "Epoch 7     loss=0.4161 [111.3 s]\tdev=(HR@5:0.4300,NDCG@5:0.3204) [4.3 s] *                         \n",
      "Epoch 8     loss=0.4022 [109.1 s]\tdev=(HR@5:0.4392,NDCG@5:0.3305) [3.8 s] *                         \n",
      "Epoch 9     loss=0.3903 [107.3 s]\tdev=(HR@5:0.4459,NDCG@5:0.3396) [4.0 s] *                         \n",
      "Epoch 10    loss=0.3799 [107.3 s]\tdev=(HR@5:0.4502,NDCG@5:0.3445) [4.0 s] *                         \n",
      "Epoch 11    loss=0.3706 [108.2 s]\tdev=(HR@5:0.4492,NDCG@5:0.3470) [7.1 s] *                         \n",
      "Epoch 12    loss=0.3624 [108.1 s]\tdev=(HR@5:0.4528,NDCG@5:0.3493) [3.8 s] *                         \n",
      "Epoch 13    loss=0.3553 [106.2 s]\tdev=(HR@5:0.4494,NDCG@5:0.3476) [3.8 s]                           \n",
      "Epoch 14    loss=0.3487 [106.0 s]\tdev=(HR@5:0.4520,NDCG@5:0.3499) [3.8 s] *                         \n",
      "Epoch 15    loss=0.3427 [106.2 s]\tdev=(HR@5:0.4507,NDCG@5:0.3520) [3.8 s] *                         \n",
      "Epoch 16    loss=0.3372 [105.1 s]\tdev=(HR@5:0.4543,NDCG@5:0.3545) [3.7 s] *                         \n",
      "Epoch 17    loss=0.3314 [105.4 s]\tdev=(HR@5:0.4530,NDCG@5:0.3542) [3.8 s]                           \n",
      "Epoch 18    loss=0.3265 [106.0 s]\tdev=(HR@5:0.4549,NDCG@5:0.3559) [3.7 s] *                         \n",
      "Epoch 19    loss=0.3214 [105.3 s]\tdev=(HR@5:0.4539,NDCG@5:0.3545) [3.7 s]                           \n",
      "Epoch 20    loss=0.3172 [105.8 s]\tdev=(HR@5:0.4513,NDCG@5:0.3539) [3.7 s]                           \n",
      "Epoch 21    loss=0.3129 [105.9 s]\tdev=(HR@5:0.4547,NDCG@5:0.3572) [3.8 s] *                         \n",
      "Epoch 22    loss=0.3083 [105.2 s]\tdev=(HR@5:0.4491,NDCG@5:0.3527) [3.7 s]                           \n",
      "Epoch 23    loss=0.3041 [106.0 s]\tdev=(HR@5:0.4523,NDCG@5:0.3552) [3.7 s]                           \n",
      "Epoch 24    loss=0.3000 [106.1 s]\tdev=(HR@5:0.4521,NDCG@5:0.3551) [3.9 s]                           \n",
      "Epoch 25    loss=0.2963 [107.2 s]\tdev=(HR@5:0.4495,NDCG@5:0.3534) [3.7 s]                           \n",
      "Epoch 26    loss=0.2927 [105.1 s]\tdev=(HR@5:0.4541,NDCG@5:0.3577) [3.7 s] *                         \n",
      "Epoch 27    loss=0.2892 [106.6 s]\tdev=(HR@5:0.4491,NDCG@5:0.3542) [3.7 s]                           \n",
      "Epoch 28    loss=0.2860 [106.7 s]\tdev=(HR@5:0.4492,NDCG@5:0.3548) [4.1 s]                           \n",
      "Epoch 29    loss=0.2824 [105.6 s]\tdev=(HR@5:0.4493,NDCG@5:0.3553) [4.0 s]                           \n",
      "Epoch 30    loss=0.2799 [105.2 s]\tdev=(HR@5:0.4495,NDCG@5:0.3554) [3.7 s]                           \n",
      "Epoch 31    loss=0.2758 [105.0 s]\tdev=(HR@5:0.4485,NDCG@5:0.3548) [3.9 s]                           \n",
      "Epoch 32    loss=0.2731 [106.3 s]\tdev=(HR@5:0.4482,NDCG@5:0.3543) [3.8 s]                           \n",
      "Epoch 33    loss=0.2703 [106.1 s]\tdev=(HR@5:0.4473,NDCG@5:0.3534) [3.8 s]                           \n",
      "Epoch 34    loss=0.2671 [105.6 s]\tdev=(HR@5:0.4447,NDCG@5:0.3521) [3.8 s]                           \n",
      "Epoch 35    loss=0.2645 [105.3 s]\tdev=(HR@5:0.4437,NDCG@5:0.3519) [3.7 s]                           \n",
      "Epoch 36    loss=0.2613 [105.5 s]\tdev=(HR@5:0.4454,NDCG@5:0.3529) [3.7 s]                           \n",
      "Epoch 37    loss=0.2592 [105.8 s]\tdev=(HR@5:0.4440,NDCG@5:0.3521) [3.7 s]                           \n",
      "Epoch 38    loss=0.2564 [105.6 s]\tdev=(HR@5:0.4428,NDCG@5:0.3513) [3.7 s]                           \n",
      "Epoch 39    loss=0.2540 [105.4 s]\tdev=(HR@5:0.4432,NDCG@5:0.3513) [6.8 s]                           \n",
      "Epoch 40    loss=0.2517 [102.9 s]\tdev=(HR@5:0.4385,NDCG@5:0.3488) [6.7 s]                           \n",
      "Epoch 41    loss=0.2496 [105.7 s]\tdev=(HR@5:0.4406,NDCG@5:0.3509) [3.7 s]                           \n",
      "Epoch 42    loss=0.2471 [105.9 s]\tdev=(HR@5:0.4401,NDCG@5:0.3521) [3.7 s]                           \n",
      "Epoch 43    loss=0.2445 [106.2 s]\tdev=(HR@5:0.4445,NDCG@5:0.3523) [3.7 s]                           \n",
      "Epoch 44    loss=0.2427 [104.0 s]\tdev=(HR@5:0.4371,NDCG@5:0.3468) [3.7 s]                           \n",
      "Epoch 45    loss=0.2403 [105.5 s]\tdev=(HR@5:0.4398,NDCG@5:0.3505) [3.8 s]                           \n",
      "Epoch 46    loss=0.2386 [106.1 s]\tdev=(HR@5:0.4384,NDCG@5:0.3487) [6.8 s]                           \n",
      "Epoch 47    loss=0.2365 [105.7 s]\tdev=(HR@5:0.4392,NDCG@5:0.3491) [3.7 s]                           \n",
      "Epoch 48    loss=0.2348 [105.6 s]\tdev=(HR@5:0.4342,NDCG@5:0.3476) [3.8 s]                           \n",
      "Epoch 49    loss=0.2325 [105.8 s]\tdev=(HR@5:0.4380,NDCG@5:0.3489) [3.8 s]                           \n",
      "Epoch 50    loss=0.2309 [106.0 s]\tdev=(HR@5:0.4353,NDCG@5:0.3474) [3.8 s]                           \n",
      "Epoch 51    loss=0.2292 [106.1 s]\tdev=(HR@5:0.4356,NDCG@5:0.3466) [3.7 s]                           \n",
      "Epoch 52    loss=0.2275 [105.9 s]\tdev=(HR@5:0.4331,NDCG@5:0.3457) [3.7 s]                           \n",
      "Epoch 53    loss=0.2256 [106.0 s]\tdev=(HR@5:0.4364,NDCG@5:0.3473) [3.7 s]                           \n",
      "Epoch 54    loss=0.2240 [105.7 s]\tdev=(HR@5:0.4357,NDCG@5:0.3479) [3.7 s]                           \n",
      "Epoch 55    loss=0.2226 [106.1 s]\tdev=(HR@5:0.4336,NDCG@5:0.3455) [3.8 s]                           \n",
      "Epoch 56    loss=0.2210 [105.9 s]\tdev=(HR@5:0.4361,NDCG@5:0.3481) [3.7 s]                           \n",
      "Epoch 57    loss=0.2198 [106.1 s]\tdev=(HR@5:0.4327,NDCG@5:0.3453) [3.8 s]                           \n",
      "Epoch 58    loss=0.2180 [105.8 s]\tdev=(HR@5:0.4310,NDCG@5:0.3444) [3.8 s]                           \n",
      "Epoch 59    loss=0.2167 [105.9 s]\tdev=(HR@5:0.4321,NDCG@5:0.3447) [3.7 s]                           \n",
      "Epoch 60    loss=0.2149 [105.7 s]\tdev=(HR@5:0.4330,NDCG@5:0.3454) [3.7 s]                           \n",
      "Epoch 61    loss=0.2139 [105.9 s]\tdev=(HR@5:0.4329,NDCG@5:0.3441) [6.7 s]                           \n",
      "Epoch 62    loss=0.2122 [102.9 s]\tdev=(HR@5:0.4315,NDCG@5:0.3445) [6.7 s]                           \n",
      "Epoch 63    loss=0.2112 [103.5 s]\tdev=(HR@5:0.4304,NDCG@5:0.3442) [6.6 s]                           \n",
      "Epoch 64    loss=0.2103 [105.8 s]\tdev=(HR@5:0.4303,NDCG@5:0.3441) [3.7 s]                           \n",
      "Epoch 65    loss=0.2087 [105.7 s]\tdev=(HR@5:0.4291,NDCG@5:0.3425) [3.7 s]                           \n",
      "Epoch 66    loss=0.2078 [106.1 s]\tdev=(HR@5:0.4279,NDCG@5:0.3421) [3.8 s]                           \n",
      "Epoch 67    loss=0.2063 [106.1 s]\tdev=(HR@5:0.4257,NDCG@5:0.3404) [3.8 s]                           \n",
      "Epoch 68    loss=0.2057 [106.1 s]\tdev=(HR@5:0.4285,NDCG@5:0.3414) [3.8 s]                           \n",
      "Epoch 69    loss=0.2045 [106.7 s]\tdev=(HR@5:0.4269,NDCG@5:0.3415) [3.7 s]                           \n",
      "Epoch 70    loss=0.2030 [107.0 s]\tdev=(HR@5:0.4277,NDCG@5:0.3413) [3.8 s]                           \n",
      "Epoch 71    loss=0.2021 [106.8 s]\tdev=(HR@5:0.4274,NDCG@5:0.3422) [3.7 s]                           \n",
      "Epoch 72    loss=0.2012 [102.4 s]\tdev=(HR@5:0.4279,NDCG@5:0.3413) [6.6 s]                           \n",
      "Epoch 73    loss=0.1997 [101.7 s]\tdev=(HR@5:0.4278,NDCG@5:0.3422) [4.1 s]                           \n",
      "Epoch 74    loss=0.1991 [107.1 s]\tdev=(HR@5:0.4251,NDCG@5:0.3396) [3.8 s]                           \n",
      "Epoch 75    loss=0.1982 [106.5 s]\tdev=(HR@5:0.4226,NDCG@5:0.3383) [3.8 s]                           \n",
      "Epoch 76    loss=0.1974 [107.0 s]\tdev=(HR@5:0.4237,NDCG@5:0.3371) [3.8 s]                           \n",
      "Epoch 77    loss=0.1964 [106.1 s]\tdev=(HR@5:0.4254,NDCG@5:0.3385) [3.8 s]                           \n",
      "Epoch 78    loss=0.1958 [106.1 s]\tdev=(HR@5:0.4236,NDCG@5:0.3375) [3.8 s]                           \n",
      "Epoch 79    loss=0.1945 [107.7 s]\tdev=(HR@5:0.4232,NDCG@5:0.3383) [3.7 s]                           \n",
      "Epoch 80    loss=0.1936 [105.2 s]\tdev=(HR@5:0.4255,NDCG@5:0.3397) [3.8 s]                           \n",
      "Epoch 81    loss=0.1932 [107.3 s]\tdev=(HR@5:0.4239,NDCG@5:0.3375) [3.8 s]                           \n",
      "Epoch 82    loss=0.1916 [107.8 s]\tdev=(HR@5:0.4194,NDCG@5:0.3350) [4.1 s]                           \n",
      "Epoch 83    loss=0.1917 [103.7 s]\tdev=(HR@5:0.4214,NDCG@5:0.3364) [3.7 s]                           \n",
      "Epoch 84    loss=0.1902 [106.2 s]\tdev=(HR@5:0.4226,NDCG@5:0.3372) [3.7 s]                           \n",
      "Epoch 85    loss=0.1905 [105.9 s]\tdev=(HR@5:0.4230,NDCG@5:0.3368) [3.7 s]                           \n",
      "Epoch 86    loss=0.1893 [106.0 s]\tdev=(HR@5:0.4216,NDCG@5:0.3365) [3.8 s]                           \n",
      "Epoch 87    loss=0.1886 [106.0 s]\tdev=(HR@5:0.4202,NDCG@5:0.3356) [3.8 s]                           \n",
      "Epoch 88    loss=0.1883 [106.2 s]\tdev=(HR@5:0.4214,NDCG@5:0.3365) [3.7 s]                           \n",
      "Epoch 89    loss=0.1872 [106.1 s]\tdev=(HR@5:0.4223,NDCG@5:0.3377) [3.8 s]                           \n",
      "Epoch 90    loss=0.1865 [108.5 s]\tdev=(HR@5:0.4184,NDCG@5:0.3346) [4.1 s]                           \n",
      "Epoch 91    loss=0.1861 [105.7 s]\tdev=(HR@5:0.4197,NDCG@5:0.3347) [4.1 s]                           \n",
      "Epoch 92    loss=0.1854 [105.9 s]\tdev=(HR@5:0.4199,NDCG@5:0.3354) [4.0 s]                           \n",
      "Epoch 93    loss=0.1845 [105.6 s]\tdev=(HR@5:0.4165,NDCG@5:0.3333) [4.1 s]                           \n",
      "Epoch 94    loss=0.1840 [105.4 s]\tdev=(HR@5:0.4147,NDCG@5:0.3322) [4.1 s]                           \n",
      "Epoch 95    loss=0.1837 [106.8 s]\tdev=(HR@5:0.4189,NDCG@5:0.3343) [4.1 s]                           \n",
      "Epoch 96    loss=0.1832 [106.2 s]\tdev=(HR@5:0.4178,NDCG@5:0.3339) [4.2 s]                           \n",
      "Epoch 97    loss=0.1823 [106.2 s]\tdev=(HR@5:0.4176,NDCG@5:0.3350) [4.1 s]                           \n",
      "Epoch 98    loss=0.1819 [105.7 s]\tdev=(HR@5:0.4176,NDCG@5:0.3340) [4.0 s]                           \n",
      "Epoch 99    loss=0.1812 [105.7 s]\tdev=(HR@5:0.4174,NDCG@5:0.3335) [4.4 s]                           \n",
      "Epoch 100   loss=0.1811 [105.6 s]\tdev=(HR@5:0.4181,NDCG@5:0.3334) [4.0 s]                           \n",
      "Epoch 101   loss=0.1805 [105.7 s]\tdev=(HR@5:0.4174,NDCG@5:0.3330) [4.0 s]                           \n",
      "\n",
      "Best Iter(dev)=   26\t dev=(HR@5:0.4541,NDCG@5:0.3577) [11150.3 s] \n",
      "/mnt/e/collegeitem/third1/machinelearning/Fuxi-alpha-in-ReChorus/ReChorus/src/models/BaseModel.py:85: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(model_path))\n",
      "Load model from /mnt/e/collegeitem/third1/machinelearning/Fuxi-alpha-in-ReChorus/models_saved/fuxi_upgrade_grocery_large.pt\n",
      "                                                                                                    \n",
      "Dev  After Training: (HR@5:0.4541,NDCG@5:0.3577,HR@10:0.5427,NDCG@10:0.3863,HR@20:0.6473,NDCG@20:0.4126,HR@50:0.8288,NDCG@50:0.4486)\n",
      "                                                                                                    \n",
      "Test After Training: (HR@5:0.4139,NDCG@5:0.3221,HR@10:0.5027,NDCG@10:0.3508,HR@20:0.6139,NDCG@20:0.3787,HR@50:0.8113,NDCG@50:0.4178)\n",
      "\n",
      "--------------------------------------------- END: 2025-12-03 03:07:55 ---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "!python src/main.py $COMMON_ML1M_L200 \\\n",
    "  --model_name fuxi_upgrade \\\n",
    "  --model_path {model_path} \\\n",
    "  --log_file {log_file} \\\n",
    "  --fuxi_blocks  8\\\n",
    "  --fuxi_heads 2 \\\n",
    "  --fuxi_attention_dim 25 \\\n",
    "  --fuxi_linear_dim 25 \\\n",
    "  --save_final_results 0\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
